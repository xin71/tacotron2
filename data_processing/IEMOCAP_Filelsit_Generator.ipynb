{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import soundfile as sf\n",
    "import shutil\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to generate file list for Tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emo_wav_generate(emotdir):\n",
    "    \"\"\"\n",
    "    this function return a dictionary where the key is the wavname and value is the extracted emotion.\n",
    "    each item is a utterance sample\n",
    "    \"\"\"\n",
    "    emot_map = {}\n",
    "    with open(emotdir, 'r') as file:\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line[0] == '[':\n",
    "                t = line.split()\n",
    "                emot_map[t[3]] = t[4]\n",
    "    return emot_map\n",
    "\n",
    "\n",
    "def generate_label(emotion): \n",
    "    if emotion == 'ang':\n",
    "        return emotion\n",
    "    elif emotion == 'sad': \n",
    "        return 'sad'\n",
    "    elif emotion == 'hap' or emotion == 'exc':\n",
    "        return 'hap'\n",
    "    elif emotion == 'neu':\n",
    "        return emotion\n",
    "    elif emotion == 'fru':\n",
    "        return emotion\n",
    "    else: # This could be SURPRISED, FEARFUL, FRUSTRATED, DISGUSTED and OTHER\n",
    "        return 'something'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session_list = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "rootdir = '/data3/IEMOCAP/IEMOCAP_full_release_spec'\n",
    "emotion_path_dict = defaultdict(list)\n",
    "path_emotion_dict = defaultdict(list)\n",
    "for session in session_list:\n",
    "    wav_dir = os.path.join(rootdir, session, 'sentences/wav/*')\n",
    "    trans_dir = os.path.join(rootdir, session, 'dialog/transcriptions')\n",
    "    for sess_path in sorted(glob.glob(wav_dir)):\n",
    "        emoevl = '/'.join(sess_path.split('/')[:-3])\n",
    "        emoevl = os.path.join(emoevl, 'dialog', 'EmoEvaluation') #/data3/IEMOCAP/IEMOCAP_full_release/Session4/dialog/EmoEvaluation\n",
    "        sess = sess_path.split('/')[-1]\n",
    "        emotdir = emoevl + '/' + sess + '.txt'\n",
    "        emotdir = emotdir.replace('IEMOCAP_full_release_spec', 'IEMOCAP_full_release')\n",
    "        trans_path = os.path.join(trans_dir, sess +'.txt')\n",
    "        emot_map = emo_wav_generate(emotdir)\n",
    "        for path in glob.glob(os.path.join(sess_path, '*.npy')):\n",
    "            filename = path.split('/')[-1].split('.')[0]\n",
    "            path = '..' + path\n",
    "            emotion = generate_label(emot_map[filename])\n",
    "            emotion_path_dict[emotion].append(path)\n",
    "            path_emotion_dict[path].append(emotion)\n",
    "\n",
    "path_emotion_dict = dict(path_emotion_dict)\n",
    "emotion_path_dict = dict(emotion_path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion:  fru\n",
      "1849\n",
      "Emotion:  something\n",
      "2659\n",
      "Emotion:  neu\n",
      "1708\n",
      "Emotion:  ang\n",
      "1103\n",
      "Emotion:  sad\n",
      "1084\n",
      "Emotion:  hap\n",
      "1636\n"
     ]
    }
   ],
   "source": [
    "for emo in list(emotion_path_dict.keys()):\n",
    "    print('Emotion: ', emo)\n",
    "    print(len(list(emotion_path_dict[emo])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../filelists/IEMOCAP/emotion_path_dict.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2d5e67a5dddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../filelists/IEMOCAP/emotion_path_dict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_path_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../filelists/IEMOCAP/path_emotion_dict.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_emotion_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../filelists/IEMOCAP/emotion_path_dict.pickle'"
     ]
    }
   ],
   "source": [
    "with open('../filelists/IEMOCAP/emotion_path_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(emotion_path_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../filelists/IEMOCAP/path_emotion_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(path_emotion_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../filelists/IEMOCAP/emotion_path_dict.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "emotion_path_dict == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUT = 12 # 12s\n",
    "SR = 16000 \n",
    "\n",
    "def check_audio_length(path):\n",
    "    audio, _ = sf.read(path)\n",
    "    if len(audio) < (CUT * SR):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def random_sample_emotion(used_path, all_path, used_emo, emo_map):\n",
    "#     if all_path == []:\n",
    "#         return None \n",
    "#     else:\n",
    "#         all_path.remove(used_path)\n",
    "#         path = random.sample(all_path, 1)[0]   \n",
    "#     while True:\n",
    "#         path_wav_name = path.split('/')[-1].split('.')[0]\n",
    "#         path_emo = emo_map[path_wav_name]\n",
    "#         print('path_emo: ', path_emo)\n",
    "#         if check_audio_length(path) == True and path_emo == used_emo:\n",
    "#             return path\n",
    "#         else:\n",
    "#             return random_sample_emotion(path, all_path, used_emo, emo_map)\n",
    "\n",
    "\n",
    "def random_sample_emotion(used_path, used_emo, emo_map):\n",
    "    all_path = list(emo_map[used_emo])\n",
    "    all_path.remove(used_path)\n",
    "    return random.sample(all_path, 1)[0]\n",
    "    \n",
    "def random_sample_speaker(used_path, all_path, gender):\n",
    "    if all_path == []:\n",
    "        return None \n",
    "    elif len(all_path) == 1:\n",
    "        path = all_path[0]\n",
    "        all_path.remove(path)\n",
    "    else:\n",
    "        all_path.remove(used_path)\n",
    "        path = random.sample(all_path, 1)[0]   \n",
    "    while True:\n",
    "        path_wav_name = path.split('/')[-1].split('.')[0]\n",
    "        path_gender = path_wav_name.split('_')[-1][0]\n",
    "        if check_audio_length(path) == True and path_gender == gender:\n",
    "            return path\n",
    "        else:\n",
    "            return random_sample_speaker(path, all_path, gender)\n",
    "\n",
    "def make_path_hyak(old_wav_npy_path, dot=False):\n",
    "    old_wav_npy_path = old_wav_npy_path.replace('IEMOCAP_full_release', 'IEMOCAP_full_release_spec')\n",
    "    old_wav_npy_path = old_wav_npy_path.replace('.wav', '.npy')\n",
    "    if dot == True:\n",
    "        new_wav_npy_path = '..' + old_wav_npy_path\n",
    "    else:\n",
    "        new_wav_npy_path = old_wav_npy_path\n",
    "    return new_wav_npy_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rootdir = '/data3/IEMOCAP/IEMOCAP_full_release'\n",
    "\n",
    "\n",
    "def emo_script(wav_dir, trans_dir):\n",
    "    trans_map = {}\n",
    "    skipped_cnt_length = 0\n",
    "    skipped_cnt_speaker_embedding = 0\n",
    "    skipped_cnt_emotion_embedding = 0\n",
    "    skipped_cnt_emo = 0\n",
    "    skipped_cnt_other = 0\n",
    "    \n",
    "    counted = 0\n",
    "    for sess_path in sorted(glob.glob(wav_dir)):\n",
    "        emoevl = '/'.join(sess_path.split('/')[:-3])\n",
    "        emoevl = os.path.join(emoevl, 'dialog', 'EmoEvaluation') #/data3/IEMOCAP/IEMOCAP_full_release/Session4/dialog/EmoEvaluation\n",
    "        sess = sess_path.split('/')[-1]\n",
    "        emotdir = emoevl + '/' + sess + '.txt'\n",
    "        trans_path = os.path.join(trans_dir, sess +'.txt')\n",
    "        emot_map = emo_wav_generate(emotdir)\n",
    "        with open(trans_path, 'r') as file:\n",
    "            while True:\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                if line[0] == 'S':\n",
    "                    split_line = line.split(':')\n",
    "                    transcript = split_line[-1][1:-2]\n",
    "                    wav_file_name = split_line[0].split()[0]\n",
    "                    wav_name = wav_file_name +'.wav'\n",
    "                    wav_path = str(os.path.join(sess_path, wav_name))\n",
    "                    try: # filter out wavname like Ses03F_impro06_MXX1\n",
    "                        # Ses03F_impro05_MXX0\n",
    "                        x = int(wav_file_name[-3:])\n",
    "                    except:\n",
    "                        skipped_cnt_other += 1\n",
    "                        continue\n",
    "            \n",
    "                    y, _ = sf.read(wav_path)\n",
    "                    if len(y) >= SR * CUT:\n",
    "                        skipped_cnt_length += 1\n",
    "                        continue\n",
    "                \n",
    "                    emotion = emot_map[wav_file_name]\n",
    "                    emotion = generate_label(emotion)\n",
    "                    \n",
    "#                     # build a new dataset with emotion name on the filename_emotion.npy\n",
    "                    old_wav_npy_path = wav_path.replace('IEMOCAP_full_release', 'IEMOCAP_full_release_spec')\n",
    "                    old_wav_npy_path = old_wav_npy_path.replace('.wav', '.npy')\n",
    "                    hyak_new_wav_path = '..' + old_wav_npy_path\n",
    "            \n",
    "                    #check if the emotion is in the range:\n",
    "                    if emotion not in ['hap', 'ang', 'neu', 'sad', 'fru']:\n",
    "                        skipped_cnt_emo += 1\n",
    "                        continue\n",
    "                    # Resample one\n",
    "                    speaker_sess_id = wav_name.split('.')[0][:-3]\n",
    "                    speaker_sess_path = str(os.path.join(sess_path, speaker_sess_id))\n",
    "                    all_same_wav_speaker = glob.glob(speaker_sess_path + '*.wav')\n",
    "                    gender = wav_file_name.split('_')[-1][0]\n",
    "                    all_same_wav_emo = glob.glob(wav_dir + '/*/*.wav')\n",
    "                    sampled_path_speaker = random_sample_speaker(wav_path, all_same_wav_speaker, gender)\n",
    "                    sampled_path_emo = random_sample_emotion(hyak_new_wav_path, emotion, emotion_path_dict)\n",
    "                    if sampled_path_speaker == None:\n",
    "                        skipped_cnt_speaker_embedding += 1\n",
    "                        continue\n",
    "                    if len(transcript) == 0:\n",
    "                        continue\n",
    "                    trans_map[hyak_new_wav_path] = [transcript, make_path_hyak(sampled_path_speaker, dot=True), \n",
    "                                                    sampled_path_emo]\n",
    "                    counted += 1\n",
    "    print('Number of Successfull files: ', counted)\n",
    "    print('Number of skipped_cnt_length: ', skipped_cnt_length)\n",
    "    print('Number of skipped_cnt_speaker_embedding: ', skipped_cnt_speaker_embedding)\n",
    "    print('Number of skipped_cnt_emo_embedding: ', skipped_cnt_emotion_embedding)\n",
    "    print('Number of skipped_cnt_emo: ', skipped_cnt_emo)\n",
    "    print('Number of skipped_cnt_other: ', skipped_cnt_other)\n",
    "    print('============================================================')\n",
    "    return trans_map\n",
    "\n",
    "def dict2list(test_dict, flag):\n",
    "    output = []\n",
    "    for key, value in test_dict.items():\n",
    "        if flag == 0:\n",
    "            s = key + '|' + value[0] + '\\n'\n",
    "        elif flag == 1: # speaker embedding\n",
    "            s = value[1] + '|' + 'nothing is here' + '\\n'\n",
    "        else: # emotion embedding\n",
    "            s = value[2] + '|' + 'nothing is here' + '\\n'\n",
    "        output.append(s)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Test Code\n",
    "# wav_dir = os.path.join(rootdir, 'Session4', 'sentences/wav/*')\n",
    "# trans_dir = os.path.join(rootdir, 'Session4', 'dialog/transcriptions')\n",
    "# trans_dict = emo_script(wav_dir, trans_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating .txt files for train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session_list = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "\n",
    "for session in session_list:\n",
    "    wav_dir = os.path.join(rootdir, session, 'sentences/wav/*')\n",
    "    trans_dir = os.path.join(rootdir, session, 'dialog/transcriptions')\n",
    "    if session == 'Session5':\n",
    "        test_dict = emo_script(wav_dir, trans_dir)\n",
    "        test_output = dict2list(test_dict, 0)\n",
    "        test_output_speaker = dict2list(test_dict, 1)\n",
    "        test_output_emotion = dict2list(test_dict, 2)\n",
    "        with open('../filelists/IEMOCAP/test_filelist.txt', 'w') as file:\n",
    "            file.writelines(test_output)\n",
    "        with open('../filelists/IEMOCAP/test_filelist_speaker.txt', 'w') as file:\n",
    "            file.writelines(test_output_speaker)\n",
    "        with open('../filelists/IEMOCAP/test_filelist_emotion.txt', 'w') as file:\n",
    "            file.writelines(test_output_emotion)\n",
    "\n",
    "    else:\n",
    "        train_dict = emo_script(wav_dir, trans_dir)\n",
    "        train_output = dict2list(train_dict, 0)\n",
    "        with open('../filelists/IEMOCAP/train_filelist.txt', 'a') as file:\n",
    "            file.writelines(train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# original_path = '../filelists/IEMOCAP'\n",
    "# for ftype in ['train', 'val', 'test']:\n",
    "#     file_path = original_path + '/' + ftype + '_filelist.txt'\n",
    "#     alt_path = original_path + '/' + ftype + '_filelist_alt.txt'\n",
    "#     with open(file_path, 'r') as a, open(alt_path, 'r') as b:\n",
    "#         content_a = a.readlines()\n",
    "#         content_b = b.readlines()\n",
    "#         for line_a, line_b in zip(content_a, content_b):\n",
    "#             path_wav_a = line_a.split('|')[0]\n",
    "#             path_wav_b = line_b.split('|')[0]\n",
    "            \n",
    "#             if path_wav_a.split('.')[0][-4] == path_wav_b.split('.')[0][-4] \\\n",
    "#                 and line_a.split('|')[0] != line_b.split('|')[0] and check_audio_length(path_wav_a) \\\n",
    "#                     and check_audio_length(path_wav_b):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 print('==============================')\n",
    "#                 print('something wrong')\n",
    "#                 print('ftype', ftype)\n",
    "#                 print(line_a.split('|')[0].split('.')[0][-4])\n",
    "#                 print(line_b.split('|')[0].split('.')[0][-4])\n",
    "#                 print(line_a.split('|')[0])\n",
    "#                 print(line_b.split('|')[0])\n",
    "#                 print('==============================')\n",
    "# print('Finish Checking')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_lines_train = sum(1 for line in open('../filelists/IEMOCAP/train_filelist.txt'))\n",
    "# num_lines_test = sum(1 for line in open('../filelists/IEMOCAP/test_filelist.txt'))\n",
    "# num_lines_test_alt = sum(1 for line in open('../filelists/IEMOCAP/test_filelist_alt.txt'))\n",
    "\n",
    "# print('Check the numbers!')\n",
    "# print(num_lines_train)\n",
    "# print(num_lines_test)\n",
    "# print('====================')\n",
    "# print(num_lines_test_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train count:  5788\n",
    "# Valid count:  2103\n",
    "# Test count:  2194\n",
    "# total count:  10085\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taco",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
